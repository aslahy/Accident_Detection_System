{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28112b0c",
   "metadata": {},
   "source": [
    "# Video Accident Detection and Intensity Classification\n",
    "\n",
    "This notebook processes videos to detect accidents using YOLO and classify their intensity using the trained ResNet18 model.\n",
    "\n",
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db84d858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Users/aslahyasar/miniconda3/envs/yolov8/lib/python3.10/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: onnxruntime in /Users/aslahyasar/miniconda3/envs/yolov8/lib/python3.10/site-packages (1.22.0)\n",
      "Requirement already satisfied: torch in /Users/aslahyasar/miniconda3/envs/yolov8/lib/python3.10/site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in /Users/aslahyasar/miniconda3/envs/yolov8/lib/python3.10/site-packages (0.22.1)\n",
      "Requirement already satisfied: pillow in /Users/aslahyasar/miniconda3/envs/yolov8/lib/python3.10/site-packages (11.0.0)\n",
      "Requirement already satisfied: numpy in /Users/aslahyasar/miniconda3/envs/yolov8/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: coloredlogs in /Users/aslahyasar/miniconda3/envs/yolov8/lib/python3.10/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/aslahyasar/miniconda3/envs/yolov8/lib/python3.10/site-packages (from onnxruntime) (25.2.10)\n",
      "Requirement already satisfied: packaging in /Users/aslahyasar/miniconda3/envs/yolov8/lib/python3.10/site-packages (from onnxruntime) (25.0)\n",
      "Requirement already satisfied: protobuf in /Users/aslahyasar/miniconda3/envs/yolov8/lib/python3.10/site-packages (from onnxruntime) (6.31.1)\n",
      "Requirement already satisfied: sympy in /Users/aslahyasar/miniconda3/envs/yolov8/lib/python3.10/site-packages (from onnxruntime) (1.13.3)\n",
      "Requirement already satisfied: filelock in /Users/aslahyasar/miniconda3/envs/yolov8/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/aslahyasar/miniconda3/envs/yolov8/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/aslahyasar/miniconda3/envs/yolov8/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/aslahyasar/miniconda3/envs/yolov8/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/aslahyasar/miniconda3/envs/yolov8/lib/python3.10/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/aslahyasar/miniconda3/envs/yolov8/lib/python3.10/site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/aslahyasar/miniconda3/envs/yolov8/lib/python3.10/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/aslahyasar/miniconda3/envs/yolov8/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install opencv-python onnxruntime torch torchvision pillow numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0a3b81",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fc48754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n",
      "OpenCV version: 4.11.0\n",
      "PyTorch version: 2.7.1\n",
      "ONNX Runtime version: 1.22.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import time\n",
    "import onnxruntime as ort\n",
    "import os\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"ONNX Runtime version: {ort.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0651ffe8",
   "metadata": {},
   "source": [
    "## 3. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe96b208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intensity classification model loaded successfully on cpu\n"
     ]
    }
   ],
   "source": [
    "# Define the corrected model architecture to match the saved weights\n",
    "class AccidentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AccidentClassifier, self).__init__()\n",
    "        # Load ResNet18 and modify it directly (without wrapping in self.resnet)\n",
    "        resnet = models.resnet18(pretrained=False)\n",
    "        \n",
    "        # Copy all layers except the final fc layer\n",
    "        self.conv1 = resnet.conv1\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.maxpool = resnet.maxpool\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        self.avgpool = resnet.avgpool\n",
    "        \n",
    "        # Replace final layer for binary classification\n",
    "        num_features = resnet.fc.in_features\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 2)  # 2 classes: weak (0), strong (1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Load intensity classification model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "intensity_model = AccidentClassifier()\n",
    "\n",
    "# Load the trained weights\n",
    "checkpoint = torch.load('accident_intensity_classifier.pth', map_location=device)\n",
    "intensity_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "intensity_model.to(device)\n",
    "intensity_model.eval()\n",
    "\n",
    "print(f\"Intensity classification model loaded successfully on {device}\")\n",
    "\n",
    "# Define transforms for intensity model\n",
    "intensity_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61208239",
   "metadata": {},
   "source": [
    "## 4. Video Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4750dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(input_video_path, output_video_path):\n",
    "    \"\"\"\n",
    "    Process video to detect accidents using ONNX model and classify intensity\n",
    "    \n",
    "    Returns:\n",
    "        frame_rate: Original video frame rate\n",
    "        avg_processing_time: Average processing time per frame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open input video\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video: {input_video_path}\")\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Video properties:\")\n",
    "    print(f\"  Frame rate: {frame_rate:.2f} FPS\")\n",
    "    print(f\"  Resolution: {width}x{height}\")\n",
    "    print(f\"  Total frames: {total_frames}\")\n",
    "    \n",
    "    # Setup output video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (width, height))\n",
    "    \n",
    "    processing_times = []\n",
    "    frame_count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Preprocess frame for ONNX model\n",
    "        input_tensor = preprocess_image(frame)\n",
    "        \n",
    "        # Run ONNX inference\n",
    "        outputs = onnx_session.run(None, {input_details.name: input_tensor})\n",
    "        \n",
    "        # Post-process detections\n",
    "        detections = postprocess_detections(outputs, frame.shape, conf_threshold=0.5)\n",
    "        \n",
    "        # Process each detection\n",
    "        for detection in detections:\n",
    "            x1, y1, x2, y2 = detection['bbox']\n",
    "            confidence = detection['confidence']\n",
    "            \n",
    "            # Extract region of interest\n",
    "            roi = frame[y1:y2, x1:x2]\n",
    "            \n",
    "            if roi.size > 0:  # Check if ROI is valid\n",
    "                # Convert to PIL Image and classify intensity\n",
    "                roi_pil = Image.fromarray(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))\n",
    "                roi_tensor = intensity_transform(roi_pil).unsqueeze(0).to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs_intensity = intensity_model(roi_tensor)\n",
    "                    probabilities = torch.softmax(outputs_intensity, dim=1)\n",
    "                    _, predicted = torch.max(outputs_intensity, 1)\n",
    "                \n",
    "                # Get prediction results\n",
    "                class_names = ['Weak', 'Strong']\n",
    "                predicted_class = class_names[predicted.item()]\n",
    "                intensity_confidence = probabilities[0][predicted.item()].item()\n",
    "                \n",
    "                # Draw bounding box\n",
    "                color = (0, 0, 255) if predicted_class == 'Strong' else (0, 255, 0)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                \n",
    "                # Add text labels\n",
    "                label = f\"Accident-{predicted_class}: {intensity_confidence:.2f}\"\n",
    "                detection_label = f\"Det: {confidence:.2f}\"\n",
    "                \n",
    "                # Draw labels\n",
    "                cv2.putText(frame, label, (x1, y1-10), \n",
    "                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "                cv2.putText(frame, detection_label, (x1, y2+20), \n",
    "                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "        \n",
    "        # Calculate processing time for this frame\n",
    "        end_time = time.time()\n",
    "        processing_times.append(end_time - start_time)\n",
    "        \n",
    "        # Write frame to output video\n",
    "        out.write(frame)\n",
    "        \n",
    "        frame_count += 1\n",
    "        if frame_count % 30 == 0:  # Progress update every 30 frames\n",
    "            print(f\"Processed {frame_count}/{total_frames} frames\")\n",
    "    \n",
    "    # Clean up\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    # Calculate average processing time\n",
    "    avg_processing_time = np.mean(processing_times)\n",
    "    \n",
    "    print(f\"\\nProcessing completed!\")\n",
    "    print(f\"Output saved to: {output_video_path}\")\n",
    "    \n",
    "    return frame_rate, avg_processing_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f9c5ac",
   "metadata": {},
   "source": [
    "## 5. Process Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8dfc742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting video processing...\n",
      "Video properties:\n",
      "  Frame rate: 30.00 FPS\n",
      "  Resolution: 1280x720\n",
      "  Total frames: 518\n",
      "Processed 30/518 frames\n",
      "Processed 60/518 frames\n",
      "Processed 90/518 frames\n",
      "Processed 120/518 frames\n",
      "Processed 150/518 frames\n",
      "Processed 180/518 frames\n",
      "Processed 210/518 frames\n",
      "Processed 240/518 frames\n",
      "Processed 270/518 frames\n",
      "Processed 300/518 frames\n",
      "Processed 330/518 frames\n",
      "Processed 360/518 frames\n",
      "Processed 390/518 frames\n",
      "Processed 420/518 frames\n",
      "Processed 450/518 frames\n",
      "Processed 480/518 frames\n",
      "Processed 510/518 frames\n",
      "\n",
      "Processing completed!\n",
      "Output saved to: output_video_with_predictions_2.mp4\n",
      "\n",
      "==================================================\n",
      "PROCESSING RESULTS\n",
      "==================================================\n",
      "Original frame rate: 30.00 FPS\n",
      "Average processing time per frame: 0.1261 seconds\n",
      "Processing speed: 7.93 FPS\n",
      "Real-time capability: No\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Define input and output paths\n",
    "input_video = \"data/testing2.mp4\"  # Replace with your video path\n",
    "output_video = \"output_video_with_predictions_2.mp4\"\n",
    "\n",
    "# Check if input video exists\n",
    "if not os.path.exists(input_video):\n",
    "    print(f\"Input video not found: {input_video}\")\n",
    "    print(\"Please update the 'input_video' variable with the correct path.\")\n",
    "else:\n",
    "    # Process the video\n",
    "    print(\"Starting video processing...\")\n",
    "    frame_rate, avg_processing_time = process_video(input_video, output_video)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PROCESSING RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Original frame rate: {frame_rate:.2f} FPS\")\n",
    "    print(f\"Average processing time per frame: {avg_processing_time:.4f} seconds\")\n",
    "    print(f\"Processing speed: {1/avg_processing_time:.2f} FPS\")\n",
    "    print(f\"Real-time capability: {'Yes' if 1/avg_processing_time >= frame_rate else 'No'}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9c7cde",
   "metadata": {},
   "source": [
    "## 6. Alternative: Custom Post-processing (if needed)\n",
    "\n",
    "If the default post-processing doesn't work well with your specific ONNX model, you can customize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd58fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_postprocess_detections(outputs, image_shape, input_size=(640, 640), conf_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Custom post-processing function - adjust based on your ONNX model's output format\n",
    "    \n",
    "    Common ONNX model output formats:\n",
    "    - YOLOv5/v8: [batch, num_detections, 85] where 85 = 4 (bbox) + 1 (conf) + 80 (classes)\n",
    "    - YOLOv3/v4: [batch, 3, grid_h, grid_w, 85]\n",
    "    \n",
    "    Adjust this function based on your model's specific output format\n",
    "    \"\"\"\n",
    "    \n",
    "    # Example for YOLOv5 format: [1, num_detections, 85]\n",
    "    if len(outputs[0].shape) == 3:\n",
    "        predictions = outputs[0][0]  # Remove batch dimension: [num_detections, 85]\n",
    "        \n",
    "        detections = []\n",
    "        for detection in predictions:\n",
    "            # Extract bbox, confidence, and class scores\n",
    "            x_center, y_center, width, height = detection[:4]\n",
    "            confidence = detection[4]\n",
    "            class_scores = detection[5:]\n",
    "            \n",
    "            # Skip low confidence detections\n",
    "            if confidence < conf_threshold:\n",
    "                continue\n",
    "            \n",
    "            # Get best class\n",
    "            class_id = np.argmax(class_scores)\n",
    "            class_score = class_scores[class_id]\n",
    "            \n",
    "            # Final confidence is obj_conf * class_conf\n",
    "            final_confidence = confidence * class_score\n",
    "            \n",
    "            if final_confidence < conf_threshold:\n",
    "                continue\n",
    "            \n",
    "            # Convert to corner coordinates and scale to image size\n",
    "            height_img, width_img = image_shape[:2]\n",
    "            scale_x = width_img / input_size[0]\n",
    "            scale_y = height_img / input_size[1]\n",
    "            \n",
    "            x1 = int((x_center - width/2) * scale_x)\n",
    "            y1 = int((y_center - height/2) * scale_y)\n",
    "            x2 = int((x_center + width/2) * scale_x)\n",
    "            y2 = int((y_center + height/2) * scale_y)\n",
    "            \n",
    "            # Ensure coordinates are within image bounds\n",
    "            x1 = max(0, min(x1, width_img-1))\n",
    "            y1 = max(0, min(y1, height_img-1))\n",
    "            x2 = max(0, min(x2, width_img-1))\n",
    "            y2 = max(0, min(y2, height_img-1))\n",
    "            \n",
    "            if x2 > x1 and y2 > y1:\n",
    "                detections.append({\n",
    "                    'bbox': [x1, y1, x2, y2],\n",
    "                    'confidence': float(final_confidence),\n",
    "                    'class_id': int(class_id)\n",
    "                })\n",
    "        \n",
    "        return detections\n",
    "    \n",
    "    # If your model has different output format, implement it here\n",
    "    else:\n",
    "        print(f\"Unsupported output format: {[out.shape for out in outputs]}\")\n",
    "        return []\n",
    "\n",
    "# Test the custom post-processing by replacing the function call in process_video\n",
    "# detections = custom_postprocess_detections(outputs, frame.shape, conf_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1500ad19",
   "metadata": {},
   "source": [
    "## 7. Display Sample Frame (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e8a02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample frame from the processed video\n",
    "def show_sample_frame(video_path, frame_number=100):\n",
    "    \"\"\"Show a sample frame from the processed video\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Go to specific frame\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        # Convert BGR to RGB for display\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(frame_rgb)\n",
    "        plt.title(f\"Sample Frame {frame_number} with Predictions\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "# Show sample frame if output video exists\n",
    "if os.path.exists(output_video):\n",
    "    show_sample_frame(output_video, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456ee82a",
   "metadata": {},
   "source": [
    "## Usage Notes\n",
    "\n",
    "**Required Files:**\n",
    "- `best.onnx` - Your trained ONNX model for accident detection\n",
    "- `accident_intensity_classifier.pth` - Your trained ResNet18 model\n",
    "- Input video file\n",
    "\n",
    "**Key Parameters to Adjust:**\n",
    "- `conf_threshold=0.5` - ONNX model detection confidence threshold\n",
    "- Post-processing function - May need adjustment based on your ONNX model's output format\n",
    "- Color coding: Red for \"Strong\", Green for \"Weak\" accidents\n",
    "\n",
    "**Output:**\n",
    "- Processed video with bounding boxes and intensity predictions\n",
    "- Frame rate of original video\n",
    "- Average processing time per frame\n",
    "- Real-time processing capability assessment\n",
    "\n",
    "**Note:** The post-processing function assumes a standard YOLO format. If your ONNX model has a different output format, use the custom post-processing function in section 6."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
